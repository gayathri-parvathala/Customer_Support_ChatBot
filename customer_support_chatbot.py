# -*- coding: utf-8 -*-
"""Customer_Support_ChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TzZTAVS0rJXhi2OCEZ8MDGg4wUlykD3U
"""

!pip install -q transformers accelerate bitsandbytes gradio
from huggingface_hub import login
login()

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import gradio as gr

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True
)
faq = {
    "track order": "You can track your order by visiting our Order Tracking page and entering your order ID.",
    "return policy": "Our return policy allows returns within 30 days of purchase.",
    "payment methods": "We accept credit/debit cards, UPI, net banking, and wallets.",
    "warranty": "All electronic items come with a 1-year manufacturer warranty.",
    "smartphone specifications": "Specifications are listed under the 'Specifications' section of the product page.",
    "laptop specifications": "Laptop specs are visible on product pages and filter options.",
    "shipping": "Free shipping on orders above â‚¹500. Standard delivery takes 3-5 business days.",
    "customer service": "Customer service is available 24/7 via chat, email, or phone.",
    "cancel order": "You can cancel your order within 24 hours through your account page.",
    "exchange": "Product exchanges are allowed within 15 days for unopened items."
}

def get_best_match(query):
    query = query.lower()
    for key in faq:
        if key in query:
            return faq[key]
    return None
def build_prompt(user_input, context, history):
    system_prompt = "You are a helpful customer support assistant for an e-commerce website."
    if context:
        system_prompt += f" Use this information: {context}"

    prompt = f"<|system|>{system_prompt}\n"
    for u, a in history[-2:]:
        prompt += f"<|user|>{u}\n<|assistant|>{a}\n"
    prompt += f"<|user|>{user_input}\n<|assistant|>"
    return prompt
def respond(message, history):
    try:
        context = get_best_match(message)
        prompt = build_prompt(message, context, history)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=150,
                do_sample=True,
                temperature=0.7,
                top_k=50,
                top_p=0.95,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        if "<|assistant|>" in response:
            response = response.split("<|assistant|>")[-1].strip()

        if len(response) < 10:
            response = context if context else "Could you please provide more details about your question?"

        return response

    except Exception as e:
        print(f"Error: {e}")
        return "Sorry, I'm experiencing technical difficulties. Please try again later."
gr.ChatInterface(
    fn=respond,
    title="Customer Support Chatbot",
    description="Ask about orders, returns, payments, specifications, shipping, and more.",
    theme="soft",
    textbox=gr.Textbox(
        placeholder="Ask a question like 'How can I track my order?'",
        container=False,
        scale=7
    ),
    chatbot=gr.Chatbot(height=500),
).launch(share=True, debug=True)